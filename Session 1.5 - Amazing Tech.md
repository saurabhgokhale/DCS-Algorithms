<!-- Link to Google Font -->
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swa" rel="stylesheet">


<span style="font-family: 'Montserrat', sans-serif;">

# Session 1.5 Amazing Tech

# ChatGPT

> What is a GPT
Generative Pre-trained Transformer

![image](https://github.com/user-attachments/assets/b7b0c6b5-eede-4037-8656-71e98e3cf962)


>Do you know how the Chat GPT works
```
Transformers are a type of a Neural Network in Machine Learning Models.

```

<img width="1000" alt="image" src="https://github.com/user-attachments/assets/d1509be9-b92b-423f-a78a-48302a4d4921">

```
There are many different types of Transformers, some take in audio and produce a transcript. 

Some take text and generate image
```


<img width="1000" alt="image" src="https://github.com/user-attachments/assets/51ffaa56-afd0-42f6-b309-4432a179dc5b">

<img width="1000" alt="image" src="https://github.com/user-attachments/assets/4a482586-3805-401b-8255-78e510793773">

```
ChatGPT transformer is designed to accept a piece of text and maybe some images or audio accompanying it and produce a prediction of what comes next in the passage.

Now, it does not feel like the chatGPT that you may have seen.

However, once you have a prediction modal like this, we can feed the same prediction data back to the model to generate the next text.
```

![image](https://github.com/user-attachments/assets/6b1dcaf0-439b-41c0-8b8f-b6ba32acd03f)

![](https://github.com/user-attachments/assets/085b7e56-3d24-4ea2-9bd0-1eda3c78c9e3)

## Embedding
```
The initial text is broken down in to smaller chunks called Tokens
```

![image](https://github.com/user-attachments/assets/6e7f9769-7f5b-4e54-9136-379e4776b400)

```
These tokens are then associated with vectors. 

```

![image](https://github.com/user-attachments/assets/f186bbd2-3b59-4036-8ec2-fd1b08c579a1)


### Attention Block

```
These sequences of vectors are then passed through a phase called Attention Block. 

```

![image](https://github.com/user-attachments/assets/e1c209d8-8507-458a-ba96-9edb7ca44a1c)

```
Here these vendors can talk to each other pass information back and forth and update their values.
For example, the meaning of the word "Model" is different in the sentence "A machine learning model" than in the phrase "fashion model".

The attention block is responsible for figuring out which words in the context are relevant in figuring out the meaning of other words.
```

![image](https://github.com/user-attachments/assets/05e22fa2-072f-4411-a932-eedd8d920069)


### Multilayer Perceptron / Feedforward block 
```
Here, the vendors don't talk to each other. Instead, they all go through the same operation in parallel. 
```

![image](https://github.com/user-attachments/assets/4e495bb2-d113-43f0-b8d4-b2ee427f491f)

```
Think about this as asking a long list of questions to each vendor and updating them based on the answers to those questions.

```

![image](https://github.com/user-attachments/assets/8080b41d-81e6-4eb6-888e-fe3d9903b49f)

```
These steps repeat over and over again until the most relevant information is added to the end of that input sentence.
```


## Golden Record Voyager

Let's talk next time.
https://www.youtube.com/watch?v=HWXHIfou3pw
</span>
